{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226818f-a5b9-4162-a201-4da5278fb1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad89a11-593f-48e1-aaeb-e30fe152430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/media/wiebke/248535ce-9ac0-4041-bcc5-7b5fd94a36bb/home/wiebke/data/' #on ps4: /data/mswc/\n",
    "mswc_metadata = data_path+'mswc/metadata.json' # get from https://mlcommons.org/en/multilingual-spoken-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c61ea-17a7-4760-9ddd-592f31214f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(mswc_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0908e-e59a-4f56-b265-1c5d6dd9945a",
   "metadata": {},
   "source": [
    "df.iloc[:,0:-1].T.sort_values(by='number_of_words', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ff939-3b5e-4139-82e0-38e048da6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_kws = dict(sorted(df['de']['wordcounts'].items(), key=lambda item: item[1], reverse=True)) \n",
    "en_kws = dict(sorted(df['en']['wordcounts'].items(), key=lambda item: item[1], reverse=True)) \n",
    "fr_kws = dict(sorted(df['fr']['wordcounts'].items(), key=lambda item: item[1], reverse=True)) \n",
    "rw_kws = dict(sorted(df['rw']['wordcounts'].items(), key=lambda item: item[1], reverse=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37fde3-f7a4-4e66-91fd-8a1dda8b9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_splits = pd.read_csv(data_path+'mswc/en_full/en_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f1fed-23e5-422b-92f5-e1768b3107e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_gender_stats = en_splits.groupby(['GENDER','SET'])['VALID'].count()#.reset_index()\n",
    "en_gender_totals = en_gender_stats.reset_index().groupby('GENDER')['VALID'].sum()#.reset_index()\n",
    "print(en_gender_stats / en_gender_totals)\n",
    "print(en_gender_stats / en_gender_stats.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c5081-cf0c-4747-83da-f6e67a970375",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_splits = pd.read_csv(data_path+'mswc/de_full/de_splits.csv') # download from https://mlcommons.org/en/multilingual-spoken-words/ - same goes for all other splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758758e-8471-4ba3-b440-491cf06a13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_gender_stats = de_splits.groupby(['GENDER','SET'])['VALID'].count()#.reset_index()\n",
    "de_gender_totals = de_gender_stats.reset_index().groupby('GENDER')['VALID'].sum()#.reset_index()\n",
    "print(de_gender_stats / de_gender_totals)\n",
    "print(de_gender_stats / de_gender_stats.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a03dbd-ee84-4119-b881-88c2ec8cc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_splits = pd.read_csv(data_path+'mswc/fr_full/fr_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320369ed-bbd4-4563-8c5e-9f58998df4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_gender_stats = fr_splits.groupby(['GENDER','SET'])['VALID'].count()#.reset_index()\n",
    "fr_gender_totals = fr_gender_stats.reset_index().groupby('GENDER')['VALID'].sum()#.reset_index()\n",
    "print(fr_gender_stats / fr_gender_totals)\n",
    "print(fr_gender_stats / fr_gender_stats.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb8186-0ee5-4ed1-b766-c9d27e5775b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_splits = pd.read_csv(data_path+'mswc/rw_full/rw_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c7656-6f56-433e-9272-160d8897eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_gender_stats = rw_splits.groupby(['GENDER','SET'])['VALID'].count()#.reset_index()\n",
    "rw_gender_totals = rw_gender_stats.reset_index().groupby('GENDER')['VALID'].sum()#.reset_index()\n",
    "print(rw_gender_stats / rw_gender_totals)\n",
    "print(rw_gender_stats / rw_gender_stats.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f175d87-492f-4d79-a31b-8f614598735c",
   "metadata": {},
   "source": [
    "Rules for constructing evaluation dataset:\n",
    "- language selection: en, de, fr, rw - most resourced languages\n",
    "- we need train, dev, test splits where male and female speakers are equally represented across keywords and where keywords are equally represented\n",
    "- all 4 languages should be equally represented\n",
    "- we want each language dataset to resemble google speech commands - so roughly 100k keywords per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50af56-3a1f-4192-a0e3-38a74ae50aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_kw_list(dataset):\n",
    "    \"\"\"\n",
    "    Filter dataset to only include keywords that meet the follwoing criteria:\n",
    "    * in top 100 most frequent occurences\n",
    "    * keyword length is greater than 3 characters\n",
    "    * if there are multiple keywords that start with the same three characters, only take the first occuring keyword\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = dataset.groupby(['WORD'])['VALID'].count().sort_values(ascending=False)[:100] #select 100 most spoken keywords\n",
    "    \n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for x in word_list.index:\n",
    "        if len(x) > 3: # only take words with more than 3 characters following MSWC paper\n",
    "            if x[:3] not in seen: # if words start with the same 3 letters, only take the first occurence\n",
    "                uniq.append(x)\n",
    "                seen.add(x[:3])\n",
    "    \n",
    "    kws_selector = np.array([w for w in word_list.index if w in uniq])\n",
    "    \n",
    "    filtered_word_list = word_list[word_list.index.isin(kws_selector)].sort_index() #subselect only those words that meet criteria\n",
    "    \n",
    "    return filtered_word_list\n",
    "\n",
    "\n",
    "\n",
    "def get_train_val_test_splits(dataset):\n",
    "    # --> partially follows protocol as described in MSWC paper\n",
    "    \n",
    "    dataset['WORD_SPEAKER'] = list(zip(dataset.WORD, dataset.SPEAKER)) \n",
    "    train_val_test_lists = {}\n",
    "    random.seed(4)\n",
    "    \n",
    "    for g in ['MALE','FEMALE']:\n",
    "        #create list of unique (keyword, speaker) pairs so that train, test and eval sets are separate \n",
    "        unique_kws_speaker_pairs = dataset[dataset.GENDER==g]['WORD_SPEAKER'].unique()    \n",
    "        #randomly sample 80% of (keyword, speaker) pairs for TRAINING\n",
    "        train_kws_speaker_pairs = random.sample(list(unique_kws_speaker_pairs), round(0.8*len(unique_kws_speaker_pairs))) \n",
    "        #randomly sample 10% of (keyword, speaker) pairs for VALIDATION, excluding pairs already in TRAINING\n",
    "        val_kws_speaker_pairs = random.sample(list(set(unique_kws_speaker_pairs).difference(set(train_kws_speaker_pairs))), round(0.1*len(unique_kws_speaker_pairs)))\n",
    "        #use the remaining (keyword, speaker) pairs for TESTING\n",
    "        test_kws_speaker_pairs = list(set(unique_kws_speaker_pairs).difference(set(train_kws_speaker_pairs).union(set(val_kws_speaker_pairs))))\n",
    "        \n",
    "        #get file links for all pairs    \n",
    "        train_val_test_lists['training_list_'+g.lower()] = list(dataset[dataset.WORD_SPEAKER.isin(train_kws_speaker_pairs)]['LINK'].values)\n",
    "        train_val_test_lists['validation_list_'+g.lower()] = list(dataset[dataset.WORD_SPEAKER.isin(val_kws_speaker_pairs)]['LINK'].values)\n",
    "        train_val_test_lists['testing_list_'+g.lower()] = list(dataset[dataset.WORD_SPEAKER.isin(test_kws_speaker_pairs)]['LINK'].values)\n",
    "    \n",
    "    return train_val_test_lists\n",
    "\n",
    "\n",
    "\n",
    "def generate_mswc_data_lists_gender_balanced(splits, n_kws):\n",
    "    \n",
    "    splits_mf = splits[splits.GENDER.isin(['MALE','FEMALE'])] #only use audio clips where gender metadata is known\n",
    "    filtered_word_list = get_filtered_kw_list(splits_mf)\n",
    "    \n",
    "    # create dataset balanced by gender across keywords\n",
    "    word_count_mf = splits_mf[splits_mf.WORD.isin(filtered_word_list.index)].groupby(['WORD','GENDER'])['VALID'].count().reset_index()\n",
    "    data_gen = []\n",
    "    for word in filtered_word_list.index:\n",
    "        counter = min(word_count_mf.loc[(word_count_mf.WORD==word)&(word_count_mf.GENDER=='FEMALE'),'VALID'].values[0],\n",
    "                    word_count_mf.loc[(word_count_mf.WORD==word)&(word_count_mf.GENDER=='MALE'),'VALID'].values[0])\n",
    "        \n",
    "        data_gen.append(splits_mf[splits_mf.WORD==word].groupby(['GENDER']).sample(n=counter, random_state=1))\n",
    "        \n",
    "    dataset = pd.concat(data_gen, axis=0, ignore_index=True)\n",
    "    kw_list = dataset.groupby(['GENDER','WORD'])['SPEAKER'].count().sort_values(ascending=False)[:n_kws*2].reset_index()['WORD'].unique()\n",
    "    dataset = dataset.loc[dataset.WORD.isin(kw_list)]\n",
    "   \n",
    "    train_val_test_dict = get_train_val_test_splits(dataset)\n",
    "    \n",
    "    assert(len(dataset)==sum([len(x) for x in train_val_test_dict.values()]))\n",
    "    \n",
    "    return train_val_test_dict, kw_list\n",
    "\n",
    "\n",
    "def save_mswc_data_lists(splits_dict, n_kws, save_dir=None):\n",
    "\n",
    "    list_dict = {}\n",
    "    \n",
    "    for k, v in splits_dict.items():\n",
    "\n",
    "        train_val_test_dict, kw_list = generate_mswc_data_lists_gender_balanced(v, n_kws)\n",
    "\n",
    "        list_dict[k] = {}    \n",
    "        list_dict[k]['kw_list'] = kw_list\n",
    "        i=0\n",
    "        for l in train_val_test_dict.keys():\n",
    "            list_dict[k][l] = train_val_test_dict[l]\n",
    "\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(save_dir+'/'+k, exist_ok=True)\n",
    "                write_to = save_dir+'/'+k+'/'+l+'.txt'\n",
    "                with open(write_to, 'w') as f:\n",
    "                    save_list = train_val_test_dict[l]\n",
    "                    random.shuffle(save_list)\n",
    "                    for line in save_list:\n",
    "                        f.write(\"%s\\n\" % line.replace(\"opus\", \"wav\"))\n",
    "                print('Saved ', write_to)\n",
    "            i+=1\n",
    "            \n",
    "    return list_dict\n",
    "\n",
    "\n",
    "def generate_commands(kw_list, n_kws):\n",
    "\n",
    "    kw_list.sort()\n",
    "    command_dict = dict(zip(kw_list, range(0, n_kws, 1)))\n",
    "    \n",
    "    return print(command_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305403f-7f18-43ed-8211-c25a168c3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kws=35\n",
    "save_dir = '/home/wiebke/data/mswc/wav_files'+str(n_kws)\n",
    "splits_dict = {'de':de_splits, 'rw':rw_splits, 'en':en_splits, 'fr':fr_splits}\n",
    "\n",
    "list_dict = save_mswc_data_lists(splits_dict, n_kws, save_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64bb22-d907-4cc8-97c9-f879b557506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_commands(list_dict['rw']['kw_list'], n_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0a417-5961-40fd-8399-40dd3dacb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "commands = dict(Counter([x.split('/')[0] for x in list_dict['rw']['validation_list_male']]))\n",
    "len(commands.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325a88f-740d-4a88-8c42-41d8b5c71cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(list_dict, language, agg_list=['count', 'nunique']):\n",
    "\n",
    "    lists = ['training_list_female', 'training_list_male', 'validation_list_female', 'validation_list_male', 'testing_list_female', 'testing_list_male']\n",
    "\n",
    "    agg_func = [pd.Series.nunique if x=='nunique' else x for x in agg_list]\n",
    "    kw_count = pd.DataFrame(index = list_dict[language]['kw_list'])\n",
    "    for l in lists:\n",
    "        df = splits_dict[language][splits_dict[language].LINK.isin(list_dict[language][l])].groupby('WORD')['SPEAKER'].agg(agg_func)\n",
    "        df.columns = [l+'_'+agg for agg in agg_list]\n",
    "        kw_count = kw_count.join(df)      \n",
    "    kw_count['total'] = kw_count.sum(axis=1)\n",
    "    kw_count.sort_values(by='total', ascending=False, inplace=True)\n",
    "\n",
    "    return kw_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940382ac-203d-44e8-8f88-708b3f89ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = (l+'_count' for l in ['training_list_female', 'training_list_male', 'validation_list_female', 'validation_list_male', 'testing_list_female', 'testing_list_male'])\n",
    "group2 = (l+'_nunique' for l in ['training_list_female', 'training_list_male', 'validation_list_female', 'validation_list_male', 'testing_list_female', 'testing_list_male'])\n",
    "\n",
    "dataset_info(list_dict, 'de',['count']).drop(['total'], axis=1).plot.bar(stacked=True, figsize=(10, 5), \n",
    "                                                                         title='Histogram of MSWC-de keywords across train, test and validation splits for males and females');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ef7dc-6155-4053-8eb9-a9c27f89a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info(list_dict, 'de',['count']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f5c9d-5745-4243-9ff3-6f34dfb22625",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info(list_dict, 'fr',['count']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d01cdd-e717-422e-8112-3a54b27be4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "(85572-75644)/75644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9079a4f-b011-467f-8443-bff90965780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info(list_dict, 'en',['count']).drop(['total'], axis=1).plot.bar(stacked=True, figsize=(10, 5),\n",
    "                                                                         title='Histogram of MSWC-en keywords across train, test and validation splits for males and females');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44293ab4-e12f-4b63-a4a2-aac8309ecb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info(list_dict, 'fr',['count']).drop(['total'], axis=1).plot.bar(stacked=True, figsize=(10, 5),\n",
    "                                                                         title='Histogram of MSWC-fr keywords across train, test and validation splits for males and females');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46c63e-5f7d-408d-8f4c-729640da3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info(list_dict, 'rw',['count']).drop(['total'], axis=1).plot.bar(stacked=True, figsize=(10, 5),\n",
    "                                                                         title='Histogram of MSWC-rw keywords across train, test and validation splits for males and females');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f437b-ee44-4366-a347-86c0f331fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function that concats all lists to give stats of dataset\n",
    "\n",
    "dataset.groupby(['GENDER','WORD'])['SPEAKER'].nunique().unstack().T.plot.barh(figsize=(25,10), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb335846-0e3f-4249-b14a-622009d48ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.WORD_SPEAKER.isin(dev_kws_speaker_pairs)].groupby(['GENDER'])[['WORD','SPEAKER', 'WORD_SPEAKER']].agg(['count','nunique'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61b39f32-9f70-421e-8233-f62084780f35",
   "metadata": {},
   "source": [
    "# bash command for copying 100 largest directories of keywords into the dataset directory\n",
    "cp -r `ls -daSh $PWD/de/clips/* | head -100` ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
